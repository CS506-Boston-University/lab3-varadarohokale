{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3: Clustering with K-means, K-means++ and Kmedioids\n",
    "\n",
    "## Overview\n",
    "In this lab, you will learn how to implement and analyze the **K-means**, **K-means++**  and **Kmedioids** clustering algorithms on two different real-world datasets:\n",
    "\n",
    "- **MNIST Digits** (10 handwritten digits (image)).  \n",
    "- **CIFAR-10 dataset** (different categories of imagenet style objects).  \n",
    "\n",
    "You will compare the performance of **K-means**,  **K-means++** and **Kmedioids**.\n",
    "\n",
    "Note that everywhere you need to use random_state = 42 where ever needed for Kmeans, Kmeans++ and Kmedioids for reproducibility, if you set this to anotehr value **you will not get full score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install datasets scikit-learn matplotlib numpy seaborn pandas\n",
    "# !pip install kmedoids\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import kmedoids\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset MNIST (HandWritten Digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Load MNIST dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:5000]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      6\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1774\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1775\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1776\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1777\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1778\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1779\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   1780\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1781\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1782\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1783\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1784\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1786\u001b[0m )\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1503\u001b[0m     path,\n\u001b[1;32m   1504\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1505\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1506\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1507\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1508\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1219\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1215\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1203\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1189\u001b[0m             path,\n\u001b[1;32m   1190\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1194\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1197\u001b[0m             path,\n\u001b[1;32m   1198\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1199\u001b[0m             data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1200\u001b[0m             data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1201\u001b[0m             download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1202\u001b[0m             download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m-> 1203\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:769\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetModule:\n\u001b[1;32m    760\u001b[0m     hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    762\u001b[0m         revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    763\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token,\n\u001b[1;32m    764\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    767\u001b[0m         sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns_in_dataset_repository(hfh_dataset_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    772\u001b[0m         patterns,\n\u001b[1;32m    773\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    774\u001b[0m         base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m    775\u001b[0m         allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    776\u001b[0m     )\n\u001b[1;32m    777\u001b[0m     split_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    778\u001b[0m         split: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    780\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:662\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[0;34m(dataset_info, base_path)\u001b[0m\n\u001b[1;32m    660\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info, base_path\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyDatasetError(\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:223\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m--> 223\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m pattern_resolver(pattern)\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    225\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/data_files.py:473\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 473\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mglob(PurePath(pattern)\u001b[38;5;241m.\u001b[39mas_posix()) \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[1;32m    474\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    475\u001b[0m     filepath\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fsspec/spec.py:613\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    611\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 613\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    614\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[1;32m    616\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    617\u001b[0m     p: info\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    626\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fsspec/utils.py:732\u001b[0m, in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m     )\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[1;32m    736\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "# 1. Load MNIST dataset\n",
    "\n",
    "dataset = load_dataset(\"mnist\", split=\"train[:5000]\")\n",
    "dataset = dataset.shuffle(seed=42) \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(4, 4))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = dataset[i][\"image\"]      \n",
    "    label = dataset.features[\"label\"].int2str(dataset[i][\"label\"])  \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label, fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess: resize + flatten images\n",
    "\n",
    "X = np.array([np.array(img.resize((64, 64))).flatten() \n",
    "              for img in dataset[\"image\"]], dtype=np.float32)\n",
    "X = X / 255.0  # normalize to [0, 1]\n",
    "\n",
    "print(\"Data shape:\", X.shape)  # (N, 64*64*3)\n",
    "\n",
    "\n",
    "# Reduce to 50 dimensions (you can tune this number)\n",
    "pca = PCA(n_components=786, random_state=42)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# 3. Extract labels\n",
    "\n",
    "y = np.array(dataset[\"label\"])\n",
    "label_names = dataset.features[\"label\"].names\n",
    "\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# 4. Count unique clusters\n",
    "\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "n_classes = len(unique_labels)\n",
    "\n",
    "print(\"\\nNumber of unique clusters (classes):\", n_classes)\n",
    "print(\"Allclasses:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"{unique_labels[i]} -> {label_names[unique_labels[i]]} ({counts[i]} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a helper function that will help you visualize the top 5 datapoints in each cluster\n",
    "def show_closest_images(X, dataset, centroids, labels, title, num_images=5):\n",
    "    n_clusters = len(centroids)\n",
    "    fig, axes = plt.subplots(n_clusters, num_images, figsize=(1.2*num_images, 1.2*n_clusters))\n",
    "\n",
    "\n",
    "    # If only 1 cluster, fix axes shape\n",
    "    if n_clusters == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances = np.linalg.norm(X - centroid, axis=1)\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue \n",
    "\n",
    "        closest_indices = cluster_indices[np.argsort(distances[cluster_indices])[:num_images]]\n",
    "\n",
    "        for j, idx in enumerate(closest_indices):\n",
    "            try:\n",
    "                img = dataset[int(idx)][\"image\"]\n",
    "            except:\n",
    "                img = dataset[int(idx)][\"img\"]\n",
    "            label_name = dataset.features[\"label\"].int2str(dataset[int(idx)][\"label\"])\n",
    "\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].axis(\"off\")\n",
    "\n",
    "            axes[i, j].text(0.5, -0.15, label_name,\n",
    "                            size=7, ha=\"center\", va=\"top\", transform=axes[i, j].transAxes)\n",
    "\n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(f\"Cluster {i}\", fontsize=9)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Implement KMeans for MNIST (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this section, you will implement **K-means** using the **sklearn** library.\n",
    "\n",
    "- Use the `KMeans` class from `sklearn.cluster`. \n",
    "- Set the following parameters:\n",
    "  - `n_clusters = k` (choose based on the dataset what do you think thge right number is for this dataset?).  \n",
    "  - `init = \"?\"` what should this be for Kmeans?.  \n",
    "  - `n_init = 10` (please set the number of random initializations to 10).  \n",
    "  - `max_iter = 50` (maximum number of iterations before convergence need to be set to 50).  \n",
    "  - `random_state = 42` (for reproducibility).  \n",
    "\n",
    "Note that everywhere you need to use random_state = 42 where ever needed for Kmeans, Kmeans++ and Kmedioids for reproducibility, if you set this to another value **you will not get full score**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Implement KMeans\n",
    "# ----------------------------\n",
    "\n",
    "k = 10\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k, \n",
    "    init=\"random\",  \n",
    "    n_init=10, \n",
    "    max_iter=50, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels_km = kmeans.fit_predict(X)\n",
    "\n",
    "centroids_km = kmeans.cluster_centers_\n",
    "\n",
    "print(\"KMeans clustering completed.\")\n",
    "print(\"Labels shape:\", labels_km.shape)\n",
    "print(\"Centroids shape:\", centroids_km.shape)\n",
    "# ----------------------------\n",
    "# Implemenetation ends here \n",
    "# ----------------------------\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "X_2d = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X)\n",
    "\n",
    "\n",
    "tick_labels = []\n",
    "for cluster_id in range(k):\n",
    "    idxs = np.where(labels_km == cluster_id)[0]\n",
    "    true_labels = y[idxs]\n",
    "    if len(true_labels) > 0:\n",
    "        counts = Counter(true_labels).most_common(2)\n",
    "        if len(counts) == 1:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]})\"\n",
    "        else:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]}), {counts[1][0]} ({counts[1][1]})\"\n",
    "        tick_labels.append(label_str)\n",
    "    else:\n",
    "        tick_labels.append(\"Empty\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Left: KMeans clusters\n",
    "sc1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_km, cmap=\"tab10\", s=10)\n",
    "cbar1 = plt.colorbar(sc1, ax=axes[0], ticks=range(k))\n",
    "cbar1.ax.set_yticklabels(tick_labels)\n",
    "axes[0].set_title(\"KMeans (2D projection), Top 2 common number in each cluster\")\n",
    "axes[0].set_xlabel(\"t-SNE 1\")\n",
    "axes[0].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "# Right: True labels\n",
    "sc2 = axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap=\"tab10\", s=10)\n",
    "cbar2 = plt.colorbar(sc2, ax=axes[1])\n",
    "cbar2.set_label(\"Digit Label\")\n",
    "axes[1].set_title(\"True Labels (2D projection)\")\n",
    "axes[1].set_xlabel(\"t-SNE 1\")\n",
    "axes[1].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO]: Find Confused Digit Pairs in Kmeans (1 point)\n",
    "\n",
    "- For each cluster, check the **second most common true label**.  \n",
    "- If its count is **greater than 50**, record the pairs (most common, second most common) and write your answer here. \n",
    "\n",
    "Answer: [TODO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_closest_images(X, dataset, centroids_km, labels_km, \"K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Implement KMeans++ for MNIST (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this section, you will implement **K-means ++** using the **sklearn** library.\n",
    "\n",
    "- Use the `KMeans` class from `sklearn.cluster`. \n",
    "- Set the following parameters:\n",
    "  - `n_clusters = k` (choose based on the dataset what do you think thge right number is for this dataset?).  \n",
    "  - `init = \"?\"` what should this be for Kmeans ++ ?.  \n",
    "  - `n_init = 10` (please set the number of random initializations to 10).  \n",
    "  - `max_iter = 50` (maximum number of iterations before convergence need to be set to 50).  \n",
    "  - `random_state = 42` (for reproducibility).  \n",
    "\n",
    "Note that everywhere you need to use random_state = 42 where ever needed for Kmeans, Kmeans++ and Kmedioids for reproducibility, if you set this to anotehr value **you will not get full score**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Implement Kmeans ++\n",
    "# ----------------------------\n",
    "k = 10\n",
    "\n",
    "kmeans_pp = KMeans(\n",
    "    n_clusters=k,\n",
    "    init=\"k-means++\",   \n",
    "    n_init=10,\n",
    "    max_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels_pp = kmeans_pp.fit_predict(X)\n",
    "centroids_pp = kmeans_pp.cluster_centers_\n",
    "\n",
    "print(\"KMeans++ clustering completed.\")\n",
    "print(\"Labels shape:\", labels_pp.shape)\n",
    "print(\"Centroids shape:\", centroids_pp.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Implemenetation ends here \n",
    "# ----------------------------\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "X_2d = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X)\n",
    "\n",
    "\n",
    "tick_labels = []\n",
    "for cluster_id in range(k):\n",
    "    idxs = np.where(labels_pp == cluster_id)[0]\n",
    "    true_labels = y[idxs]\n",
    "    if len(true_labels) > 0:\n",
    "        counts = Counter(true_labels).most_common(2)\n",
    "        if len(counts) == 1:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]})\"\n",
    "        else:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]}), {counts[1][0]} ({counts[1][1]})\"\n",
    "        tick_labels.append(label_str)\n",
    "    else:\n",
    "        tick_labels.append(\"Empty\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Left: KMeans clusters\n",
    "sc1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_pp, cmap=\"tab10\", s=10)\n",
    "cbar1 = plt.colorbar(sc1, ax=axes[0], ticks=range(k))\n",
    "cbar1.ax.set_yticklabels(tick_labels)\n",
    "axes[0].set_title(\"KMeans++ (2D projection), Top 2 common number in each cluster\")\n",
    "axes[0].set_xlabel(\"t-SNE 1\")\n",
    "axes[0].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "# Right: True labels\n",
    "sc2 = axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap=\"tab10\", s=10)\n",
    "cbar2 = plt.colorbar(sc2, ax=axes[1])\n",
    "cbar2.set_label(\"Digit Label\")\n",
    "axes[1].set_title(\"True Labels (2D projection)\")\n",
    "axes[1].set_xlabel(\"t-SNE 1\")\n",
    "axes[1].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO]: Find Confused Digit Pairs  in Kmeans++ (1 point)\n",
    "\n",
    "- For each cluster, check the **second most common true label**.  \n",
    "- If its count is **greater than 50**, record the pair (most common, second most common) and write your answer here. \n",
    "\n",
    "Answer: Confused Digit Pairs in KMeans++ (count > 50): [(1, 7), (4, 9), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "show_closest_images(X, dataset, centroids_pp, labels_pp, \"K-means++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Implement Kmedioids (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "KMedoids (with `kmedoids` package)\n",
    "\n",
    "- Use the `KMedoids` class from the **`kmedoids`** package, which you must have pip installed at the start of the notebook.  \n",
    "- Set the following parameters:\n",
    "  - `n_clusters = k` → number of clusters (choose based on dataset).  \n",
    "  - `method = \"fasterpam\"` → efficient implementation of the PAM algorithm.  \n",
    "  - `metric = \"precomputed\"` → pass a precomputed distance matrix.  \n",
    "  - `random_state = 42` → ensures reproducibility.  \n",
    "Note that everywhere you need to use random_state = 42 where ever needed for Kmeans, Kmeans++ and Kmedioids for reproducibility, if you set this to anotehr value **you will not get full score**. \n",
    "\n",
    "Extra Note:  Before running KMedoids, you need to compute a **distance matrix** using e.g.  \n",
    "  ```python\n",
    "dist_matrix = pairwise_distances(X, metric=\"manhattan\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# TODO Implement KMedoids\n",
    "# ------------------------\n",
    "\n",
    "k = 10\n",
    "\n",
    "kmed = KMedoids(\n",
    "    n_clusters=k,\n",
    "    method=\"fasterpam\",\n",
    "    metric=\"precomputed\", \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "kmed.fit(dist_matrix)\n",
    "\n",
    "labels_kmed = kmed.labels_\n",
    "medoids_indices = kmed.medoid_indices_\n",
    "\n",
    "print(\"KMedoids clustering completed.\")\n",
    "print(\"Labels shape:\", labels_kmed.shape)\n",
    "print(\"Medoids indices:\", medoids_indices)\n",
    "\n",
    "# ----------------------------\n",
    "# Implementation ends here\n",
    "# ----------------------------\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "X_2d = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X)\n",
    "\n",
    "\n",
    "tick_labels = []\n",
    "for cluster_id in range(k):\n",
    "    idxs = np.where(labels_kmed == cluster_id)[0]\n",
    "    true_labels = y[idxs]\n",
    "    if len(true_labels) > 0:\n",
    "        counts = Counter(true_labels).most_common(2)\n",
    "        if len(counts) == 1:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]})\"\n",
    "        else:\n",
    "            label_str = f\"{counts[0][0]} ({counts[0][1]}), {counts[1][0]} ({counts[1][1]})\"\n",
    "        tick_labels.append(label_str)\n",
    "    else:\n",
    "        tick_labels.append(\"Empty\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Left: KMeans clusters\n",
    "sc1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=labels_kmed, cmap=\"tab10\", s=10)\n",
    "cbar1 = plt.colorbar(sc1, ax=axes[0], ticks=range(k))\n",
    "cbar1.ax.set_yticklabels(tick_labels)\n",
    "axes[0].set_title(\"KMeans++ (2D projection), Top 2 common number in each cluster\")\n",
    "axes[0].set_xlabel(\"t-SNE 1\")\n",
    "axes[0].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "# Right: True labels\n",
    "sc2 = axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap=\"tab10\", s=10)\n",
    "cbar2 = plt.colorbar(sc2, ax=axes[1])\n",
    "cbar2.set_label(\"Digit Label\")\n",
    "axes[1].set_title(\"True Labels (2D projection)\")\n",
    "axes[1].set_xlabel(\"t-SNE 1\")\n",
    "axes[1].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Find Confused Digit Pairs  in Kmedioids (1 point)\n",
    "\n",
    "- For each cluster, check the **second most common true label**.  \n",
    "- If its count is **greater than 50**, record the pair (most common, second most common) and write your answer here. \n",
    "\n",
    "Answer: Confused Digit Pairs in KMedoids (count > 50): [(1, 7), (4, 9), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Compute Silhouette scores for MNIST (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# TODO Implement Metrics: Compute Silhouette scores for Kmeans, Kmeans++ and Kmedioids\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "sil_km_rand = silhouette_score(X, labels_km, metric=\"euclidean\")\n",
    "\n",
    "sil_km_pp = silhouette_score(X, labels_pp, metric=\"euclidean\")\n",
    "\n",
    "sil_kmed = silhouette_score(dist_matrix, labels_kmed, metric=\"precomputed\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Implemenetation ends here \n",
    "# ----------------------------\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Algorithm\": [\"KMeans\", \"KMeans++\", \"KMedoids\"],\n",
    "    \"Silhouette Score\": [sil_km_rand, sil_km_pp, sil_kmed]\n",
    "})\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Using the Silhouette scores answer which method is doing better? (1 point)\n",
    "\n",
    "\n",
    "Answer: Based on the Silhouette scores, the algorithm with the highest score is performing better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering a pixels in a Single Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO] Image Clustering with KMeans and KMeans++ (2 points)\n",
    "\n",
    "In this exercise, you will explore how clustering can be applied to images.  \n",
    "Instead of clustering entire datasets, we cluster the **pixels** of an image based on their RGB values.  \n",
    "\n",
    "Instructions for Kmeans:\n",
    "\n",
    "- Use the `KMeans` class from `sklearn.cluster`. \n",
    "- Set the following parameters:\n",
    "  - `n_clusters = k` (set this to 5).  \n",
    "  - `init = \"?\"` what should this be for Kmeans?.  \n",
    "  - `n_init = 10` (please set the number of random initializations to 10).  \n",
    "  - `max_iter = 50` (maximum number of iterations before convergence need to be set to 50).  \n",
    "  - `random_state = 42` (for reproducibility).  \n",
    "\n",
    "Instructions for Kmeans ++\n",
    "\n",
    "- Use the `KMeans` class from `sklearn.cluster`. \n",
    "- Set the following parameters:\n",
    "  - `n_clusters = k` (choose based on the dataset what do you think thge right number is for this dataset?).  \n",
    "  - `init = \"?\"` what should this be for Kmeans ++ ?.  \n",
    "  - `n_init = 10` (please set the number of random initializations to 10).  \n",
    "  - `max_iter = 50` (maximum number of iterations before convergence need to be set to 50).  \n",
    "  - `random_state = 42` (for reproducibility). \n",
    "\n",
    "Note that everywhere you need to use random_state = 42 where ever needed for Kmeans, Kmeans++ and Kmedioids for reproducibility, if you set this to another value **you will not get full score**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from skimage.io import imread\n",
    "from skimage.transform import rescale\n",
    "import kmedoids\n",
    "\n",
    "\n",
    "image = imread(\"image0.png\")   \n",
    "image = rescale(image, 0.5, channel_axis=-1)  \n",
    "\n",
    "image_shape = image.shape\n",
    "pixels = image.reshape(-1, image_shape[-1])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TODO Implement KMeans \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "k = 5\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k,\n",
    "    init=\"random\",\n",
    "    n_init=10,\n",
    "    max_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels = kmeans.fit_predict(pixels)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "quantized = centers_rand[labels_rand].reshape(image_shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TODO KMeans++ \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "kmeans_pp = KMeans(\n",
    "    n_clusters=k,\n",
    "    init=\"k-means++\",\n",
    "    n_init=10,\n",
    "    max_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "labels_pp = kmeans.fit_predict(pixels)\n",
    "centers_pp= kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "quantized_pp = centers_pp[labels_pp].reshape(image_shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Implementation Ends here\n",
    "# ------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(quantized_rand)\n",
    "axes[1].set_title(\"KMeans\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(quantized_pp)\n",
    "axes[2].set_title(\"KMeans++\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## [TODO] Image Clustering (Instructions) (10 points)\n",
    "1. Load each of the three images which should be given to you (image1.jpg, image2.jpg, image3.jpg)\n",
    "2. For each image, apply **KMeans clustering** twice:  \n",
    "   - **KMeans (Random Init)**  \n",
    "   - **KMeans++ (Smart Init)**  \n",
    "3. Replace each pixel with the color of its cluster centroid.  \n",
    "   - This reduces the number of unique colors in the image.  \n",
    "   - The result is a “posterized” image with simplified colors.  \n",
    "4. Display the results in a **3×3 grid**:  \n",
    "   - **Column 1** → Original images.  \n",
    "   - **Column 2** → KMeans results.  \n",
    "   - **Column 3** → KMeans++ results.  \n",
    "\n",
    "You need to get the output as follows: \n",
    "- A single figure with 9 panels (3 rows × 3 columns).  \n",
    "- Each row corresponds to one image.  \n",
    "- The leftmost image in each row is the original, followed by its KMeans and KMeans++ clustered versions.  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cluster_image(image_path, k=5):\n",
    "    image = imread(image_path)\n",
    "    image = rescale(image, 0.5, channel_axis=-1)  \n",
    "    h, w, c = image.shape\n",
    "    pixels = image.reshape(-1, c)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Implement Kmeans \n",
    "    # ----------------------------------------------\n",
    "\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init=\"random\",\n",
    "        n_init=10,\n",
    "        max_iter=50,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(pixels)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    kmeans_image = centers[labels].reshape(h, w, c)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Implement Kmeans ++\n",
    "    # ----------------------------------------------\n",
    "\n",
    "    kmeans_pp = KMeans(\n",
    "        n_clusters=k,\n",
    "        init=\"k-means++\",\n",
    "        n_init=10,\n",
    "        max_iter=50,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels_pp = kmeans_pp.fit_predict(pixels)\n",
    "    centers_pp = kmeans_pp.cluster_centers_\n",
    "    kmeans_pp_image = centers_pp[labels_pp].reshape(h, w, c)\n",
    "    # ----------------------------------------------\n",
    "    # Implementation ends here \n",
    "    # ----------------------------------------------\n",
    "\n",
    "    return image, kmeans_image, kmeans_pp_image\n",
    "\n",
    "\n",
    "images = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]\n",
    "results = [cluster_image(img, k=6) for img in images]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for i, (orig, km_rand, km_pp) in enumerate(results):\n",
    "    axes[i, 0].imshow(orig)\n",
    "    axes[i, 0].set_title(f\"Original {i+1}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(km_rand)\n",
    "    axes[i, 1].set_title(f\"KMeans {i+1}\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(km_pp)\n",
    "    axes[i, 2].set_title(f\"KMeans++ {i+1}\")\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Answer the following questions (8 points)\n",
    "\n",
    "---\n",
    "\n",
    "**1. Question:** What does the Silhouette Score of KMeans++ for MNIST dataset represent?  \n",
    "\n",
    "**Answer:**  The Silhouette Score measures how well the clusters are separated and how compact each cluster is. For KMeans++ on MNIST, it quantifies how similar each digit’s embedding is to its own cluster compared to other clusters. A higher score indicates that digits of the same class are grouped closely together and clusters are well-separated.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**2. Question:** When you used the KMeans library you set `n_init = 10`. What does `n_init` do?  \n",
    "\n",
    "**Answer:**  It specifies the number of times KMeans will run with different random initializations of the centroids. The algorithm chooses the best clustering result among these runs to improve reliability.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. Question:** When you used the KMeans library you set `max_iter = 50`. What does `max_iter` do?  \n",
    "\n",
    "**Answer:**  It sets the maximum number of iterations for a single run of KMeans to update cluster centroids and assignments. If convergence is not reached before this limit, the algorithm stops to prevent infinite loops.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**4. Question:** What distance metric did you use for computing the Silhouette score of KMedoids?  \n",
    "\n",
    "**Answer:**  For KMedoids, we used the precomputed Manhattan (L1) distance matrix as the distance metric when computing the Silhouette score, which measures distances between points based on the sum of absolute differences across dimensions.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
